[
  {
    "question": "Pregunta 1: Estás diseñando un pipeline de datos en Dataflow que procesa datos de telemetría de dispositivos IoT. Necesitas enriquecer estos datos en tiempo real con información de perfil de usuario almacenada en Cloud Bigtable para personalizar alertas. ¿Cuál es el enfoque más eficiente y de menor latencia para realizar este enriquecimiento dentro de Dataflow?",
    "options": [
      "a) Usar una transformación ParDo que realice lecturas directas a Bigtable para cada elemento.",
      "b) Cargar los perfiles relevantes en una vista lateral (side input) de Dataflow y realizar un lookup.",
      "c) Escribir los datos de telemetría a Pub/Sub y usar una Cloud Function para enriquecerlos desde Bigtable.",
      "d) Usar BigQuery como lookup leyendo la tabla de perfiles directamente desde Dataflow."
    ],
    "answer": "Respuesta correcta: a",
    "explanation": "Explicación: Para lookups de baja latencia y alto volumen contra Bigtable desde Dataflow, realizar lecturas directas usando el conector de Bigtable dentro de un ParDo es el patrón más eficiente. Las vistas laterales son adecuadas para datos que cambian con poca frecuencia y caben en memoria del worker. Usar Cloud Functions añade latencia y complejidad innecesaria. BigQuery no está diseñado para lookups de clave/valor de baja latencia como Bigtable."
  },
  {
    "question": "Pregunta 2: Tu equipo ha entrenado un modelo de clasificación de texto usando Vertex AI Custom Training. Quieres desplegar este modelo para predicciones online con baja latencia y necesitas que escale automáticamente según la demanda. ¿Qué opción de despliegue en Vertex AI es la más adecuada?",
    "options": [
      "a) Vertex AI Batch Prediction job.",
      "b) Desplegar el modelo en un Vertex AI Endpoint con escalado automático configurado.",
      "c) Exportar el modelo y servirlo desde una instancia de Compute Engine con un balanceador de carga.",
      "d) Usar BigQuery ML para importar y servir el modelo."
    ],
    "answer": "Respuesta correcta: b",
    "explanation": "Explicación: Vertex AI Endpoints está diseñado específicamente para servir modelos de ML para predicciones online (en tiempo real), ofreciendo escalado automático basado en el tráfico y gestión integrada. Batch Prediction es para inferencia asíncrona sobre grandes datasets. Servir manualmente desde GCE requiere gestionar la infraestructura y el escalado. BigQuery ML sirve modelos entrenados dentro de BQ, no modelos personalizados de Vertex AI directamente para predicción online de baja latencia."
  },
  {
    "question": "Pregunta 3: Necesitas migrar un data warehouse on-premises de 50 TB a BigQuery. La migración debe completarse en una ventana de tiempo limitada y con mínima interrupción. Los datos están actualmente en formato CSV. ¿Qué combinación de servicios y herramientas de GCP es la más apropiada para esta tarea?",
    "options": [
      "a) Usar gsutil para subir los CSV a Cloud Storage y luego bq load desde GCS.",
      "b) Utilizar Storage Transfer Service para mover los CSV a Cloud Storage y luego BigQuery Data Transfer Service para cargar los datos.",
      "c) Usar Transfer Appliance para enviar físicamente los datos a Google, cargarlos en GCS y luego bq load.",
      "d) Configurar una conexión VPN o Interconnect y usar Dataflow para leer los CSV on-premises y escribirlos en BigQuery."
    ],
    "answer": "Respuesta correcta: b",
    "explanation": "Explicación: Storage Transfer Service está optimizado para transferencias a gran escala desde fuentes como almacenamiento on-premises a Cloud Storage. Una vez en GCS, BigQuery Data Transfer Service (o bq load) puede cargar eficientemente los datos en BigQuery. gsutil puede ser lento para 50 TB. Transfer Appliance es más adecuado para cientos de TB o petabytes, o cuando el ancho de banda es muy limitado. Dataflow añade complejidad innecesaria para una carga masiva inicial."
  },
  {
    "question": "Pregunta 4: Estás construyendo un pipeline de Dataflow en modo streaming que consume mensajes de Pub/Sub. El pipeline realiza agregaciones basadas en ventanas de tiempo. Quieres asegurar que los datos que llegan tarde (late data) sean procesados y actualicen los resultados previamente calculados. ¿Qué configuración de windowing y triggering",
    "options": [
      "deberías usar?",
      "a) Ventanas fijas (Fixed windows) con el trigger por defecto.",
      "b) Ventanas deslizantes (Sliding windows) con un trigger AfterWatermark.",
      "c) Ventanas de sesión (Session windows) con un trigger AfterProcessingTime."
    ],
    "answer": "d) Ventanas fijas (Fixed windows) con un trigger AfterWatermark.pastEndOfWindow().withLateFirings(AfterCount(1)) y especificando AllowedLateness.",
    "explanation": "Respuesta correcta: d"
  },
  {
    "question": "Pregunta 5: (IA) Tu empresa quiere implementar un sistema de detección de anomalías en logs de aplicaciones en tiempo real. Los logs se envían a Pub/Sub. Quieres usar un modelo de ML para identificar patrones inusuales. ¿Qué combinación de servicios de GCP sería más efectiva para construir y operar este sistema?",
    "options": [
      "a) Dataflow para procesar logs, BigQuery ML para entrenar un modelo de K-means y predicción en batch.",
      "b) Cloud Logging para ingestar logs, exportarlos a BigQuery, y usar un modelo de Vertex AI entrenado para predicción en batch.",
      "c) Pub/Sub para ingesta, Dataflow para procesamiento en tiempo real, y un modelo de detección de anomalías",
      "desplegado en un Vertex AI Endpoint para predicción en tiempo real."
    ],
    "answer": "d) Cloud Functions para procesar logs de Pub/Sub y llamar a la API de Cloud Natural Language para análisis de sentimiento.",
    "explanation": "Respuesta correcta: c"
  },
  {
    "question": "Pregunta 6: Necesitas orquestar un workflow complejo que incluye tareas en",
    "options": [
      "Dataproc (Spark), BigQuery (SQL) y Vertex AI (entrenamiento de modelo). El workflow debe ejecutarse diariamente y tener capacidades de reintento y monitoreo. ¿Qué servicio de GCP es el más adecuado para esta orquestación?",
      "a) Cloud Scheduler + Cloud Functions",
      "b) Cloud Composer (Airflow gestionado)",
      "c) Vertex AI Pipelines"
    ],
    "answer": "d) Cloud Build",
    "explanation": "Respuesta correcta: b"
  },
  {
    "question": "Pregunta 7: Estás almacenando datos de series temporales en BigQuery y frecuentemente consultas rangos de fechas específicos y filtras por un ID de dispositivo. ¿Cómo deberías diseñar la tabla para optimizar el rendimiento y el costo de estas consultas?",
    "options": [
      "a) Crear una tabla estándar sin ninguna optimización especial.",
      "b) Particionar la tabla por la columna de timestamp (ej. diaria) y clusterizarla por la columna device_id.",
      "c) Usar una tabla externa apuntando a archivos CSV en Cloud Storage.",
      "d) Crear índices secundarios en las columnas de timestamp y device_id."
    ],
    "answer": "Respuesta correcta: b",
    "explanation": "Explicación: Particionar por la columna de tiempo (timestamp) permite a BigQuery escanear solo las particiones relevantes para el rango de fechas consultado,"
  },
  {
    "question": "Pregunta 8: (IA) Quieres entrenar un modelo de regresión en BigQuery ML para predecir el valor de vida del cliente (CLV) usando datos históricos de transacciones. Los datos contienen columnas con información personal identificable (PII) que no deben ser usadas directamente en el entrenamiento pero sí quieres retenerlas en la tabla original. ¿Cuál es la mejor práctica?",
    "options": [
      "a) Crear una vista (VIEW) que excluya las columnas PII y usarla en la sentencia CREATE MODEL.",
      "b) Usar la opción TRANSFORM en CREATE",
      "MODEL para seleccionar explícitamente las características (features) deseadas, excluyendo las PII.",
      "c) Aplicar enmascaramiento dinámico de datos a las columnas PII antes de ejecutar CREATE MODEL."
    ],
    "answer": "d) Usar Cloud DLP para anonimizar las columnas PII directamente en la tabla base antes del entrenamiento.",
    "explanation": "Respuesta correcta: b"
  },
  {
    "question": "Pregunta 9: Tienes un pipeline de Dataflow que lee desde Pub/Sub y escribe en BigQuery. Durante picos de tráfico, observas que el \"data freshness\" (actualidad de los datos en BigQuery) aumenta significativamente, indicando retrasos. El número de workers de Dataflow parece estar escalando correctamente. ¿Cuál podría ser una causa común de este cuello de botella y cómo lo mitigarías?",
    "options": [
      "a) Límites de cuota de la API de Streaming Inserts de BigQuery. Mitigar solicitando un aumento de cuota.",
      "b) Latencia alta en la red entre los workers de Dataflow y la API de BigQuery. Mitigar eligiendo una región más cercana.",
      "c) El tópico de Pub/Sub no puede manejar la tasa de publicación. Mitigar",
      "aumentando las particiones del tópico."
    ],
    "answer": "d) Transformaciones ineficientes en el pipeline de Dataflow. Mitigar optimizando el código del pipeline.",
    "explanation": "Respuesta correcta: a"
  },
  {
    "question": "Pregunta 10: (IA) Estás desarrollando un modelo de recomendación para un sitio de e-commerce usando Vertex AI. Necesitas",
    "options": [
      "almacenar y servir características (features) precalculadas de usuarios y productos con baja latencia para la inferencia online. ¿Qué componente de Vertex AI deberías utilizar para gestionar y servir estas características?",
      "a) Vertex AI Model Registry",
      "b) Vertex AI Feature Store",
      "c) Vertex AI Matching Engine"
    ],
    "answer": "d) Vertex AI TensorBoard",
    "explanation": "Respuesta correcta: b"
  },
  {
    "question": "Pregunta 11: Necesitas configurar el acceso a un bucket de Cloud Storage para que un equipo específico de Data Scientists pueda leer y escribir objetos, pero no puedan eliminar el bucket ni cambiar sus permisos. Otro equipo de Data Analysts solo debe poder leer objetos. ¿Qué combinación de roles de IAM es la más apropiada siguiendo el principio de mínimo privilegio?",
    "options": [
      "a) Data Scientists: roles/storage.admin, Data Analysts: roles/storage.objectViewer.",
      "b) Data Scientists: roles/storage.objectAdmin, Data Analysts: roles/storage.objectViewer.",
      "c) Data Scientists: roles/storage.legacyBucketWriter, Data Analysts: roles/storage.legacyBucketReader.",
      "d) Data Scientists: roles/storage.objectCreator y roles/storage.objectViewer, Data Analysts: roles/"
    ],
    "answer": "storage.objectViewer.",
    "explanation": "Respuesta correcta: b"
  },
  {
    "question": "Pregunta 12: Estás operando un clúster de Dataproc para trabajos ETL de Spark. Quieres optimizar los costos asegurándote de que el clúster escale hacia abajo cuando no esté en uso activo, pero quieres evitar perder nodos primarios que mantienen el estado de HDFS si es",
    "options": [
      "posible. ¿Qué configuración deberías usar?",
      "a) Usar solo instancias preemptivas para todos los nodos del clúster.",
      "b) Configurar el Autoscaling de Dataproc especificando un número mínimo de workers primarios y permitiendo escalar workers secundarios, posiblemente preemptivos.",
      "c) Crear un clúster efímero para cada trabajo usando la acción dataproc clusters create y delete."
    ],
    "answer": "d) Aumentar manualmente el tamaño del clúster antes de trabajos grandes y reducirlo después.",
    "explanation": "Respuesta correcta: b"
  },
  {
    "question": "Pregunta 13: (IA) Has desplegado un modelo de detección de fraude en un Vertex AI Endpoint. Quieres monitorear continuamente la distribución de las características de entrada de las predicciones online y recibir alertas si esta distribución difiere significativamente de la distribución de los datos de entrenamiento (detectar deriva de características). ¿Qué capacidad de Vertex AI deberías habilitar?",
    "options": [
      "a) Vertex AI Explainable AI",
      "b) Vertex AI Model Monitoring con detección de deriva (drift detection).",
      "c) Vertex AI Continuous Evaluation",
      "d) Configurar métricas personalizadas en Cloud Monitoring sobre los logs de predicción."
    ],
    "answer": "Respuesta correcta: b",
    "explanation": "Explicación: Vertex AI Model Monitoring está diseñado explícitamente para detectar deriva (drift) en las características de entrada y/o en las predicciones de los modelos desplegados en Endpoints. Puedes configurarlo para comparar las distribuciones de las solicitudes de predicción online con una distribución de referencia (normalmente, los datos de entrenamiento) y generar alertas si se supera un umbral de deriva especificado. Explainable AI explica predicciones individuales. Continuous Evaluation evalúa métricas de rendimiento"
  },
  {
    "question": "Pregunta 14: Estás diseñando un sistema para ingestar eventos de clickstream desde una aplicación móvil a BigQuery con baja latencia. El volumen de eventos puede variar significativamente. ¿Qué secuencia de servicios de GCP es la más robusta y escalable para este caso de uso?",
    "options": [
      "a) App Móvil -> Cloud Functions -> BigQuery (Tabledata.insertAll API)",
      "b) App Móvil -> Pub/Sub -> Dataflow (Streaming) -> BigQuery (Streaming Inserts)",
      "c) App Móvil -> Cloud Storage (archivos por lotes) -> BigQuery Data Transfer Service",
      "d) App Móvil -> App Engine -> Task Queue -> BigQuery"
    ],
    "answer": "Respuesta correcta: b",
    "explanation": "Explicación: Pub/Sub actúa como un buffer escalable y desacoplado, absorbiendo picos de ingesta. Dataflow proporciona un motor de procesamiento en streaming robusto y escalable que puede realizar transformaciones si es necesario antes de escribir en BigQuery usando la API de Streaming Inserts, que está diseñada para baja latencia. Cloud Functions puede tener limitaciones de escalabilidad y concurrencia para alto volumen. El enfoque por lotes (c) no cumple el requisito de baja latencia. App Engine/Task Queue (d) añade complejidad innecesaria comparado con Pub/Sub/Dataflow."
  },
  {
    "question": "Pregunta 15: Necesitas asegurar que los datos sensibles (ej. números de tarjeta de crédito) en una tabla de BigQuery sean automáticamente detectados y anonimizados (ej. enmascarados o tokenizados) antes de que un analista de",
    "options": [
      "datos, que no debería ver los datos originales, pueda consultarlos. ¿Qué combinación de servicios y características de GCP deberías usar?",
      "a) Cloud IAM para restringir el acceso a la tabla.",
      "b) Encriptación a nivel de columna en BigQuery.",
      "c) Cloud DLP para crear plantillas de desidentificación y políticas de inspección, combinadas con enmascaramiento dinámico de datos en BigQuery o creando vistas autorizadas con los datos transformados por DLP."
    ],
    "answer": "d) Escribir un UDF (User Defined Function) en BigQuery que realice el enmascaramiento durante la consulta.",
    "explanation": "Respuesta correcta: c"
  },
  {
    "question": "Pregunta 16: (IA) Estás utilizando Vertex AI Pipelines para orquestar un flujo de trabajo de MLOps que incluye preprocesamiento de datos, entrenamiento de modelo, evaluación y despliegue condicional. Necesitas pasar un pequeño artefacto (ej. el ID de un dataset preprocesado en GCS) desde un",
    "options": [
      "paso de preprocesamiento a un paso de entrenamiento. ¿Cómo se maneja típicamente el paso de artefactos y parámetros entre componentes en Vertex AI Pipelines?",
      "a) Escribir los artefactos/parámetros en un archivo en un bucket de GCS compartido y leerlo en el siguiente paso.",
      "b) Usar variables de entorno definidas en la configuración del pipeline.",
      "c) Definir Outputs (Artefactos o Parámetros) en el componente de origen y usarlos como Inputs en el componente de destino, gestionados automáticamente por la plataforma de pipelines."
    ],
    "answer": "d) Almacenar los metadatos en una tabla de BigQuery y consultarla en el siguiente paso.",
    "explanation": "Respuesta correcta: c"
  },
  {
    "question": "Pregunta 17: Tienes un conjunto de datos en BigQuery que contiene información de clientes, incluyendo direcciones de correo electrónico. Debes compartir una versión de este conjunto de datos con un tercero",
    "options": [
      "para análisis, pero sin revelar las direcciones de correo electrónico reales. Quieres reemplazarlas con un identificador único y consistente para cada dirección, pero que no sea reversible. ¿Qué técnica de desidentificación deberías aplicar usando Cloud DLP?",
      "a) Enmascaramiento (Masking)",
      "b) Redacción (Redaction)",
      "c) Tokenización Criptográfica (Cryptographic Tokenization) usando un método de Hashing Criptográfico Seguro (como SHA-256)."
    ],
    "answer": "d) Tokenización Preservadora de Formato (Format-Preserving Tokenization) con una clave envuelta.",
    "explanation": "Respuesta correcta: c"
  },
  {
    "question": "Pregunta 18: Estás ejecutando un pipeline de Dataflow que realiza una unión (join) entre un stream de eventos grande proveniente de Pub/Sub y un conjunto de datos de referencia relativamente pequeño (menos de 1 GB) que se actualiza diariamente y está almacenado en Cloud Storage. ¿Cuál es la forma más eficiente de realizar esta unión en Dataflow?",
    "options": [
      "a) Usar una transformación CoGroupByKey entre las dos PCollections.",
      "b) Cargar el conjunto de datos de referencia como una vista lateral (side",
      "input) y realizar el lookup en un ParDo que procesa el stream principal.",
      "c) Escribir ambos flujos a BigQuery y realizar la unión allí."
    ],
    "answer": "d) Leer el archivo de referencia desde GCS para cada elemento del stream dentro de un ParDo.",
    "explanation": "Respuesta correcta: b"
  },
  {
    "question": "Pregunta 19: (IA) Entrenaste un modelo de clasificación de imágenes con Vertex AI AutoML Vision. Ahora quieres integrar este modelo en una aplicación móvil Android/iOS para que realice clasificaciones directamente en el dispositivo, incluso sin conexión a internet. ¿Qué formato de exportación de modelo deberías elegir en Vertex AI y qué tecnología usarías en la app móvil?",
    "options": [
      "a) Exportar como SavedModel de TensorFlow y usar TensorFlow Serving en un servidor.",
      "b) Exportar como modelo TensorFlow Lite (.tflite) y usar la librería TensorFlow Lite en la aplicación móvil.",
      "c) Exportar como contenedor Docker y ejecutarlo en Cloud Run.",
      "d) Exportar como ONNX y usar ONNX Runtime en la app móvil."
    ],
    "answer": "Respuesta correcta: b",
    "explanation": "Explicación: Para la inferencia en el dispositivo (on-device inference) en móviles, TensorFlow Lite (.tflite) es el formato optimizado proporcionado por Google. Vertex AI AutoML Vision permite exportar modelos en este formato. La librería TensorFlow Lite se integra fácilmente en aplicaciones Android e iOS para ejecutar estos modelos localmente, permitiendo baja latencia y funcionamiento offline. Las otras opciones implican inferencia en servidor (a, c) o un formato diferente (d) que podría ser compatible pero TFLite es el estándar para AutoML Vision en móviles."
  },
  {
    "question": "Pregunta 20: Necesitas almacenar metadatos sobre tus tablas en BigQuery, modelos de ML en Vertex AI y pipelines de datos en Dataflow en una ubicación centralizada y permitir el descubrimiento y la gobernanza de estos activos. ¿Qué servicio de GCP está diseñado para este",
    "options": [
      "propósito?",
      "a) Cloud Logging",
      "b) Cloud Monitoring",
      "c) Dataplex"
    ],
    "answer": "d) BigQuery (usando tablas de metadatos personalizadas)",
    "explanation": "Respuesta correcta: c"
  },
  {
    "question": "Pregunta 21: Un pipeline de Dataflow que",
    "options": [
      "lee de Kafka (gestionado externamente) y escribe a BigQuery falla intermitentemente con errores de OutOfMemoryError en los workers. El pipeline realiza una transformación GroupByKey que agrupa grandes cantidades de datos por clave. ¿Cuál es la causa más probable y la mejor solución?",
      "a) Las claves tienen una distribución muy sesgada (hot keys). Solución: Aplicar una transformación Combine.perKey antes del GroupByKey si es posible, o usar una técnica de \"salting\" (añadir un sufijo aleatorio a la clave).",
      "b) El tamaño de la ventana es demasiado grande. Solución: Reducir la duración de la ventana.",
      "c) La cuota de escritura de BigQuery es insuficiente. Solución: Solicitar aumento de cuota."
    ],
    "answer": "d) Los workers de Dataflow tienen un tipo de máquina demasiado pequeño.",
    "explanation": "Solución: Especificar un tipo de máquina con más memoria."
  },
  {
    "question": "Pregunta 22: (IA) Quieres usar BigQuery ML para entrenar un modelo de clasificación binaria para predecir la probabilidad de que un cliente haga clic en un anuncio. Tienes datos históricos con muchas características categóricas y numéricas. ¿Qué tipo de modelo (MODEL_TYPE) deberías especificar probablemente en tu sentencia CREATE MODEL?",
    "options": [
      "a) LINEAR_REG",
      "b) LOGISTIC_REG",
      "c) KMEANS",
      "d) AUTOML_CLASSIFIER"
    ],
    "answer": "Respuesta correcta: b",
    "explanation": "Explicación: La regresión logística (LOGISTIC_REG) es un modelo estándar y adecuado para problemas de clasificación binaria (predecir una de dos clases, como clic/no-clic). Maneja bien características"
  },
  {
    "question": "Pregunta 23: Tu organización tiene políticas estrictas sobre la residencia de datos, exigiendo que ciertos conjuntos de datos en BigQuery permanezcan físicamente dentro de la Unión Europea. Al crear un nuevo conjunto de datos para esta información, ¿qué configuración es crucial especificar?",
    "options": [
      "a) La clase de almacenamiento (Storage Class).",
      "b) La ubicación del conjunto de datos (Dataset Location), seleccionando una región o multi-región dentro de la UE (ej. europe-west1 o EU).",
      "c) Las etiquetas (Labels) del conjunto de datos.",
      "d) La clave de encriptación gestionada por el cliente (CMEK), asegurándose de que la clave esté en la UE."
    ],
    "answer": "Respuesta correcta: b",
    "explanation": "Explicación: La ubicación (Location) de un conjunto de datos de BigQuery determina la geografía física donde se almacenan los datos. Para cumplir con los requisitos de residencia de datos, es fundamental seleccionar una región o multi-región que se encuentre dentro de los límites geográficos permitidos (en este caso, la UE). Las otras configuraciones (clase de almacenamiento, etiquetas, CMEK) son importantes para otros aspectos, pero no controlan directamente la residencia física de los datos."
  },
  {
    "question": "Pregunta 24: Estás diseñando un pipeline para procesar archivos de video subidos a Cloud Storage. El pipeline debe extraer el",
    "options": [
      "audio, transcribirlo a texto usando la API de Speech-to-Text, y luego analizar el sentimiento del texto usando la API de Natural Language. Quieres orquestar estos pasos de forma robusta. ¿Qué servicio sería el más adecuado para definir y ejecutar este workflow?",
      "a) Cloud Functions encadenadas mediante Pub/Sub.",
      "b) Un script complejo ejecutándose en una instancia de Compute Engine.",
      "c) Cloud Composer (Airflow) definiendo un DAG con operadores para GCS, Speech-to-Text y Natural Language."
    ],
    "answer": "d) Dataflow con un pipeline batch.",
    "explanation": "Respuesta correcta: c"
  },
  {
    "question": "Pregunta 25: (IA) Necesitas anonimizar nombres de personas en un gran volumen de texto no estructurado almacenado en archivos dentro de Cloud Storage antes de que sean analizados. Quieres reemplazar los nombres con el tipo de entidad detectada (ej. \"[PERSON_NAME]\"). ¿Qué combinación de servicios usarías?",
    "options": [
      "a) Dataflow para leer los archivos y BigQuery ML para detectar y reemplazar nombres.",
      "b) Cloud Functions activadas por GCS para llamar a la API de Cloud Natural Language",
      "para detectar entidades.",
      "c) Usar la API de Cloud Data Loss Prevention (DLP) con una acción replaceWithInfoType configurada en un job de inspección sobre el bucket de GCS."
    ],
    "answer": "d) Entrenar un modelo NER personalizado en Vertex AI y aplicarlo con un job de Batch Prediction.",
    "explanation": "Respuesta correcta: c"
  },
  {
    "question": "Pregunta 26: Tienes una tabla muy grande en BigQuery particionada por fecha de ingesta. Las consultas que filtran por un ID de cliente específico (customer_id) son lentas porque escanean particiones enteras. ¿Qué optimización adicional deberías aplicar a la tabla para acelerar estas consultas?",
    "options": [
      "a) Cambiar la partición a customer_id.",
      "b) Clusterizar la tabla por la columna customer_id.",
      "c) Crear una vista materializada filtrada por customer_id.",
      "d) Usar la API de BigQuery Storage Read para leer los datos más rápido."
    ],
    "answer": "Respuesta correcta: b",
    "explanation": "Explicación: Dado que la tabla ya está particionada (presumiblemente por una"
  },
  {
    "question": "Pregunta 27: Quieres programar la ejecución de un script de Python que realiza una consulta en BigQuery y guarda los resultados en un bucket de Cloud Storage cada lunes a las 3 AM. El script es",
    "options": [
      "relativamente simple y de corta duración. ¿Cuál es la forma más sencilla y rentable de implementar esta tarea programada en GCP?",
      "a) Configurar un cron job en una instancia de Compute Engine siempre encendida.",
      "b) Usar Cloud Composer con un DAG simple.",
      "c) Usar Cloud Scheduler para activar una Cloud Function que ejecute el script de Python."
    ],
    "answer": "d) Desplegar el script como un servicio en Cloud Run y activarlo con Cloud Scheduler.",
    "explanation": "Respuesta correcta: c"
  },
  {
    "question": "Pregunta 28: (IA) Estás construyendo un pipeline de MLOps usando Vertex AI Pipelines. Quieres asegurarte de que un modelo solo se despliegue en producción si su métrica de precisión en un conjunto de datos de evaluación supera un umbral predefinido (ej. 90%). ¿Cómo implementarías esta lógica condicional dentro del pipeline?",
    "options": [
      "a) Usar un componente Conditional basado en un parámetro de salida del paso de evaluación del modelo.",
      "b) Escribir un script personalizado que verifique la métrica y llame a la API de despliegue si se cumple la condición.",
      "c) Configurar una política de IAM que",
      "restrinja el despliegue basado en métricas."
    ],
    "answer": "d) Usar Cloud Monitoring para monitorear la métrica y activar el despliegue a través de una alerta.",
    "explanation": "Respuesta correcta: a"
  },
  {
    "question": "Pregunta 29: Necesitas proporcionar acceso temporal a un contratista externo para que pueda cargar archivos en un",
    "options": [
      "bucket específico de Cloud Storage durante las próximas 48 horas. No quieres crear una cuenta de Google permanente para el contratista. ¿Cuál es la forma más segura y adecuada de conceder este acceso?",
      "a) Crear una cuenta de servicio, generar una clave JSON y enviársela al contratista.",
      "b) Usar URLs firmadas (Signed URLs) de Cloud Storage con un tiempo de expiración corto para la acción de carga (PUT o POST).",
      "c) Añadir la dirección de correo personal del contratista al IAM del bucket con el rol roles/storage.objectCreator."
    ],
    "answer": "d) Configurar una política de firma de V4 (V4 Signed Policy Document) que permita cargas durante 48 horas.",
    "explanation": "Respuesta correcta: d"
  },
  {
    "question": "Pregunta 30: Estás analizando datos de rendimiento de consultas en BigQuery usando las vistas INFORMATION_SCHEMA.JOBS_BY_*. Observas que muchas consultas tienen un alto valor en total_slot_ms pero un bajo total_bytes_processed. ¿Qué indica esto probablemente sobre esas consultas y qué",
    "options": [
      "podrías investigar para optimizarlas?",
      "a) Las consultas están leyendo muchos datos pequeños pero realizando operaciones de cómputo muy intensivas (ej. UDFs complejos, JOINs explosivos, expresiones regulares costosas). Investigar el plan de consulta y optimizar las etapas de cómputo.",
      "b) Las consultas están esperando recursos (slots) y pasan mucho tiempo en cola. Investigar la concurrencia y considerar reservas de slots.",
      "c) Las consultas están leyendo grandes cantidades de datos desde tablas externas. Investigar si se pueden materializar en tablas nativas."
    ],
    "answer": "d) Hay un problema con la recolección de estadísticas de la tabla, lo que lleva a planes de consulta subóptimos. Ejecutar ANALYZE TABLE.",
    "explanation": "Respuesta correcta: a"
  },
  {
    "question": "Pregunta 31: (IA) Quieres entrenar un modelo de detección de objetos en",
    "options": [
      "imágenes usando Vertex AI, pero tienes un presupuesto limitado para el etiquetado manual de datos. Solo una pequeña fracción de tu gran conjunto de imágenes está etiquetada con cuadros delimitadores (bounding boxes). ¿Qué estrategia de entrenamiento en Vertex AI podría ayudarte a aprovechar tanto los datos etiquetados como los no etiquetados para mejorar el rendimiento del modelo?",
      "a) Usar Vertex AI AutoML Vision Object Detection, ya que maneja automáticamente conjuntos de datos parcialmente etiquetados.",
      "b) Ignorar los datos no etiquetados y entrenar solo con la pequeña fracción etiquetada.",
      "c) Implementar un enfoque de aprendizaje semi-supervisado (semi-supervised learning) usando un trabajo de entrenamiento personalizado (Custom Training) en Vertex AI."
    ],
    "answer": "d) Usar BigQuery ML para entrenar el",
    "explanation": "modelo, ya que tiene capacidades semi-supervisadas incorporadas."
  },
  {
    "question": "Pregunta 32: Necesitas transferir datos",
    "options": [
      "diariamente desde una base de datos MySQL on-premises a una tabla en BigQuery. Quieres una solución gestionada por Google que maneje el esquema, la carga incremental y la programación. ¿Qué servicio de GCP deberías usar?",
      "a) Escribir un script personalizado con mysqldump y bq load.",
      "b) Usar Dataflow con conectores JDBC y BigQueryIO.",
      "c) Configurar BigQuery Data Transfer Service con el conector para Cloud SQL (requiere replicación previa a Cloud SQL) o usar un conector de terceros si es directo desde on-prem."
    ],
    "answer": "d) Usar Datastream para replicar cambios de MySQL a Cloud Storage y luego cargar desde GCS a BigQuery.",
    "explanation": "Respuesta correcta: d"
  },
  {
    "question": "Pregunta 33: Estás almacenando archivos Parquet en un bucket de Cloud Storage y quieres consultarlos directamente usando BigQuery sin cargarlos en el almacenamiento nativo de BigQuery. ¿Qué tipo de tabla deberías crear en BigQuery?",
    "options": [
      "a) Una tabla estándar de BigQuery.",
      "b) Una tabla externa (External Table).",
      "c) Una vista materializada (Materialized View).",
      "d) Una vista (View) lógica."
    ],
    "answer": "Respuesta correcta: b",
    "explanation": "Explicación: Las tablas externas en BigQuery permiten consultar datos almacenados en Cloud Storage (en formatos como Parquet, ORC, Avro, CSV, JSON) directamente, sin necesidad de cargar o duplicar los datos en el almacenamiento gestionado de BigQuery. Simplemente defines la tabla apuntando a la ubicación de los archivos en GCS y especificando el formato."
  },
  {
    "question": "Pregunta 34: (IA) Has entrenado un modelo de lenguaje grande (LLM) personalizado en Vertex AI para generar resúmenes de texto. Quieres evaluar la calidad de los resúmenes generados utilizando métricas estándar como ROUGE. ¿Cómo puedes integrar esta evaluación en tu flujo de trabajo de MLOps",
    "options": [
      "en Vertex AI?",
      "a) Utilizar las métricas incorporadas de Vertex AI Model Monitoring.",
      "b) Crear un componente personalizado en Vertex AI Pipelines que calcule ROUGE scores comparando los resúmenes generados con resúmenes de referencia.",
      "c) Usar Vertex AI Explainable AI para obtener puntuaciones de calidad."
    ],
    "answer": "d) Exportar los resultados a BigQuery y calcular ROUGE usando SQL UDFs.",
    "explanation": "Respuesta correcta: b"
  },
  {
    "question": "Pregunta 35: Necesitas implementar un pipeline de datos que procese información sensible sujeta a GDPR. El pipeline se ejecutará en Dataflow. ¿Qué medidas de seguridad son fundamentales aplicar para proteger los datos en tránsito y asegurar que el procesamiento ocurra dentro de una red controlada?",
    "options": [
      "a) Usar CMEK para encriptar los datos en las fuentes y destinos (ej. Pub/Sub, GCS, BigQuery).",
      "b) Ejecutar los workers de Dataflow dentro de una red VPC compartida y usar Private",
      "Google Access.",
      "c) Deshabilitar el acceso a IP públicas en los workers de Dataflow y configurar VPC Service Controls para crear un perímetro de seguridad alrededor de los servicios utilizados (Dataflow, GCS, BigQuery, Pub/Sub)."
    ],
    "answer": "d) Confiar en la encriptación en tránsito predeterminada de Google y enfocarse solo en los permisos IAM.",
    "explanation": "Respuesta correcta: c"
  },
  {
    "question": "Pregunta 36: Tu equipo está utilizando Cloud Composer para orquestar pipelines ETL. El entorno de Composer está experimentando lentitud y fallos en la ejecución de tareas. Quieres diagnosticar el problema. ¿Qué herramientas y métricas dentro de GCP deberías revisar primero?",
    "options": [
      "a) Los logs de las tareas de Airflow en Cloud Logging y las métricas de utilización de CPU/Memoria del clúster GKE de",
      "Composer en Cloud Monitoring.",
      "b) Las métricas de latencia de la API de BigQuery.",
      "c) Los logs de auditoría de Cloud Storage."
    ],
    "answer": "d) Las métricas de rendimiento de la red VPC.",
    "explanation": "Respuesta correcta: a"
  },
  {
    "question": "Pregunta 37: (IA) Estás usando Vertex AI Feature Store para servir características online. Observas que la latencia de obtención de características (online_serving_latency) ha aumentado. ¿Qué factor es MENOS probable que contribuya directamente a este aumento de latencia?",
    "options": [
      "a) El tamaño total del Feature Store (número de entidades o características).",
      "b) La complejidad del tipo de datos de las características que se están sirviendo.",
      "c) El número de solicitudes de servicio online por segundo (QPS).",
      "d) La distancia geográfica entre la aplicación cliente y la región del endpoint de servicio online."
    ],
    "answer": "Respuesta correcta: a",
    "explanation": "Explicación: Vertex AI Feature Store está diseñado para escalar horizontalmente y proporcionar baja latencia independientemente del tamaño total del almacén (número de entidades o características almacenadas). La latencia de servicio online se ve más afectada por la carga actual (QPS) (c), la complejidad de los datos que se recuperan por solicitud (b), y la latencia de red entre el cliente y el servidor (d). Si bien un tamaño extremo podría tener efectos indirectos, no es un factor directo en la latencia de una solicitud individual como los otros."
  },
  {
    "question": "Pregunta 38: Quieres crear una copia de seguridad de una tabla importante de BigQuery cada día y retener las copias de seguridad durante 30 días. ¿Cuál es la forma más idiomática y eficiente de lograr esto en BigQuery?",
    "options": [
      "a) Programar un job de exportación de la",
      "tabla a Cloud Storage cada día y configurar una política de ciclo de vida en el bucket.",
      "b) Usar la funcionalidad de instantáneas de tabla (Table Snapshots) de BigQuery, creando una instantánea diaria y eliminando las que tengan más de 30 días.",
      "c) Ejecutar una consulta CREATE TABLE AS SELECT (CTAS) cada día para copiar la tabla."
    ],
    "answer": "d) Usar BigQuery Data Transfer Service para copiar la tabla a otro conjunto de datos diariamente.",
    "explanation": "Respuesta correcta: b"
  },
  {
    "question": "Pregunta 39: Un pipeline de Dataflow que lee de Pub/Sub y realiza agregaciones basadas en ventanas de tiempo necesita garantizar que cada elemento se procese exactamente una vez, incluso si los workers fallan y se reinician. ¿Qué modo de procesamiento debería habilitar Dataflow y qué característica de Pub/Sub ayuda a lograr esto?",
    "options": [
      "a) Modo \"At-Least-Once\" en Dataflow y reconocimiento (ack) de mensajes en Pub/",
      "Sub.",
      "b) Modo \"Exactly-Once\" (procesamiento efectivo exactly-once) en Dataflow, que se basa en el almacenamiento persistente de Dataflow y el reconocimiento (ack) de mensajes en Pub/Sub.",
      "c) Usar Dataflow SQL con semántica exactly-once."
    ],
    "answer": "d) Implementar lógica de duplicación personalizada en el pipeline.",
    "explanation": "Respuesta correcta: b"
  },
  {
    "question": "Pregunta 40: (IA) Estás evaluando dos modelos de clasificación entrenados en Vertex AI para un problema de detección de spam. El Modelo A tiene mayor precisión general, pero el Modelo B tiene un 'recall' (sensibilidad) mucho mayor para la clase 'spam'. Dado que es crítico identificar la mayor cantidad posible de correos spam, incluso a costa de algunos falsos positivos, ¿qué métrica y qué modelo deberías priorizar?",
    "options": [
      "a) Priorizar la precisión y el Modelo A.",
      "b) Priorizar el 'recall' (sensibilidad) para la clase 'spam' y el Modelo B.",
      "c) Priorizar la puntuación F1 y elegir el modelo con la mejor puntuación F1.",
      "d) Priorizar el AUC (Área bajo la curva ROC) y elegir el modelo con el mayor AUC."
    ],
    "answer": "Respuesta correcta: b",
    "explanation": "Explicación: El 'recall' (o sensibilidad) mide la proporción de positivos reales (correos spam) que el modelo identificó correctamente. Si el objetivo principal es minimizar los falsos negativos (spam no detectado), entonces el 'recall' para la clase positiva ('spam') es la métrica más importante. Por lo tanto, se debe priorizar el Modelo B, que tiene un mayor 'recall' para el spam, aceptando potencialmente una menor precisión (más falsos positivos). La precisión se enfoca en cuán correctas son las predicciones positivas. F1 es un balance entre precisión y 'recall'."
  }
]
